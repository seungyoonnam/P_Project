{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5MrxEPmKfqh1",
        "outputId": "4687090a-5928-479a-d00e-262d8c686fc3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully installed gast-0.4.0 keras-2.13.1 tensorboard-2.13.0 tensorflow-2.13.1 tensorflow-estimator-2.13.0 tensorflow-text-2.13.0\n"
          ]
        }
      ],
      "source": [
        "pip install -U \"tensorflow-text==2.13.*\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PECQv-_kfrbG"
      },
      "outputs": [],
      "source": [
        "pip install \"tf-models-official==2.13.*\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "QiA9c8UmXAJm"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "import shutil\n",
        "import os\n",
        "import zipfile\n",
        "\n",
        "import json"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O1h2V7KJCWTt"
      },
      "source": [
        "아이디어\n",
        "1. NonClickBait, ClickBait 이진분류\n",
        "본문의 제목, 본문만 학습해 이진분류를 진행한\n",
        "\n",
        "2. 주제 자동감지 다중분류\n",
        "Topic을 참고해 클래스를 나누어 제목과 본문을 학습시키고 기사 입력시 주제 자동감지"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bFwcNuV_ZOOW"
      },
      "source": [
        "### **1. 데이터 준비**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "juPEpLQ7ZSPl"
      },
      "source": [
        "내 Google Drive에서 데이터 다운로드"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "bux00SbrZwae",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "outputId": "e562fadd-65c1-4f95-e578-56b0e3581e70"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/146.낚시성 기사 탐지 데이터.zip'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "drive.mount('/content/drive')\n",
        "\n",
        "source_path = '/content/drive/MyDrive/ML_Data/146.낚시성 기사 탐지 데이터.zip'\n",
        "destination_path = '/content/146.낚시성 기사 탐지 데이터.zip'\n",
        "shutil.copyfile(source_path,destination_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vFhUQgQPlTdF"
      },
      "source": [
        " zip 압축 풀기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "6enJlL2IfSsL"
      },
      "outputs": [],
      "source": [
        "#한글파일 unzip시 깨짐 encoding decoding으로 방지하는 함수\n",
        "def unzip(source_path, dest_path):\n",
        "  with zipfile.ZipFile(source_path, 'r') as zf:\n",
        "    zipInfo = zf.infolist()\n",
        "    for member in zipInfo:\n",
        "      try:\n",
        "        #print(member.filename.encode('cp437').decode('euc-kr','ignore'))\n",
        "        member.filename = member.filename.encode('cp437').decode('euc-kr','ignore')\n",
        "        zf.extract(member,dest_path)\n",
        "      except:\n",
        "        print(source_path)\n",
        "        raise Exception('??')\n",
        "\n",
        "zip_file_path = '/content/146.낚시성 기사 탐지 데이터.zip'\n",
        "unzip(zip_file_path,'/content')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2M6Jmp3ulVoO"
      },
      "source": [
        "**[경로]** 내부의 모든 zip file 해당 위치에 압축 해제"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "oc4Et0tKla1B"
      },
      "outputs": [],
      "source": [
        "raw_data_path1 = '/content/146.낚시성 기사 탐지 데이터/01.데이터/Training/02.라벨링데이터'\n",
        "raw_data_path2 = '/content/146.낚시성 기사 탐지 데이터/01.데이터/Validation/02.라벨링데이터'\n",
        "for p in [raw_data_path1,raw_data_path2]:\n",
        "  for root, dirs, files in os.walk(p):\n",
        "    for file_name in files:\n",
        "      if file_name.endswith('.zip'):\n",
        "        start_path = os.path.join(root,file_name)\n",
        "        dest_path = os.path.join(root,file_name[:-4])\n",
        "        if os.path.exists(dest_path):\n",
        "          pass\n",
        "        else:\n",
        "          os.mkdir(dest_path)\n",
        "        unzip(start_path,dest_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TQWjl9MIs7PO"
      },
      "source": [
        "### **2. 데이터 탐색**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8bpC_LDniKas"
      },
      "source": [
        "**기본 데이터 특징**\n",
        "\n",
        "\n",
        "*   원천데이터에는 Part1 Clickbait의 Direct와 Auto가 누락되어있음\n",
        "*   항목 추가\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nsItqZQHoDz2"
      },
      "source": [
        "**partNum**P1, P2 그냥 파트 정보\n",
        "**useType** 0낚시성 1비낚시성\n",
        "**processType** A자동생성 D직접생성"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LRFiG_T1s_nR"
      },
      "source": [
        "**processPattern (json-sourceDataInfo - processPattern)**\n",
        "*   Part1은 11부터16까지, Part2는 21부터24까지.\n",
        "*   Part1이든 Part2이든 Clickbait_Direct 에만 processPattern이 유의미한 값을 가짐. Direct(직접생성)이 아닌 Auto(자동생성)이면 NonClickbait은 00 Clickbait은 99값을 가짐\n",
        "\n",
        "11. 의문 유발형(부호)\n",
        "12. 의문 유발형(은닉)\n",
        "13. 선정표현 사용형\n",
        "14. 속어\n",
        "15. 사실과대표현\n",
        "16. 의도적 주어 왜곡\n",
        "21. 상품 판매정보\n",
        "22. 부동산 판매정보\n",
        "23. 서비스 판매정보\n",
        "24. 의도적 상황 왜곡/전환\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LFwiG4jHk7Ed",
        "outputId": "01aa607d-ac59-4916-dfb8-6ebe1b340215"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{}"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#folder_path의 json파일 다 읽어서 param 개수 저장\n",
        "folder_path = '/content/146.낚시성 기사 탐지 데이터/01.데이터/Training/01.원천데이터/TS_Part2_Clickbait_Auto_EC'\n",
        "param = 'processType'\n",
        "all_param = {}\n",
        "\n",
        "for root, dirs, files in os.walk(folder_path):\n",
        "  for file_name in files:\n",
        "    if file_name.endswith('.json'):\n",
        "      with open(root+'/'+file_name,'r') as f:\n",
        "        data=json.load(f)\n",
        "        #json 데이터 key값으로 찾는 방법\n",
        "        res = data.get('sourceDataInfo').get(param)\n",
        "\n",
        "        all_param[res] = all_param.get(res,0) + 1\n",
        "\n",
        "all_param"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JpKrrGX2jFct"
      },
      "source": [
        "**원천 데이터와 라벨링 데이터 차이**\n",
        "\n",
        "*   라벨링 데이터에 **labeledDataInfo**가 추가되어있다는 것만 다르다\n",
        "*   빌어먹을 Part1의 Clickbait 데이터가 원천데이터에는 없고 라벨링데이터에만 있다\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 276
        },
        "id": "MCfYLaVUafSW",
        "outputId": "f414e5b8-d76e-42ee-8c95-a29466808e11"
      },
      "outputs": [
        {
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-649fb9fe9f13>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#json파일 여는 방법\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_path1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mrf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m   \u001b[0mraw_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabeled_path1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mlf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/146.낚시성 기사 탐지 데이터/01.데이터/Training/01.원천데이터/TS_Part2_Clickbait_Direct_EC/EC_M02_617507.json'"
          ]
        }
      ],
      "source": [
        "#원천데이터와 라벨링데이터의 같은 이름의 데이터가 어떻게 다른지 탐색\n",
        "raw_path1 = '/content/146.낚시성 기사 탐지 데이터/01.데이터/Training/01.원천데이터/TS_Part2_Clickbait_Direct_EC/EC_M02_617507.json'\n",
        "labeled_path1 = '/content/146.낚시성 기사 탐지 데이터/01.데이터/Training/02.라벨링데이터/TL_Part2_Clickbait_Direct_EC/EC_M02_617507_L.json'\n",
        "\n",
        "#json파일 여는 방법\n",
        "with open(raw_path1,'r') as rf:\n",
        "  raw_data = json.load(rf)\n",
        "with open(labeled_path1,'r') as lf:\n",
        "  labeled_data = json.load(lf)\n",
        "\n",
        "print(raw_data)\n",
        "print()\n",
        "print(labeled_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NjmujepQrX1k"
      },
      "source": [
        "원천데이터의 newsTitle 과 라벨링데이터의 newTitle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EBRgyRmqm_jA"
      },
      "outputs": [],
      "source": [
        "from IPython.display import clear_output\n",
        "import time\n",
        "\n",
        "labeled_path2 = '/content/146.낚시성 기사 탐지 데이터/01.데이터/Training/02.라벨링데이터/TL_Part2_Clickbait_Direct_PO'\n",
        "file_list = []\n",
        "for root, dirs, files in os.walk(labeled_path2):\n",
        "  for file_name in files:\n",
        "    if file_name.endswith('.json'):\n",
        "      file_list.append(root+'/'+file_name)\n",
        "\n",
        "for i in range(60,4000):\n",
        "  #i = int(input(\"숫자 :\"))\n",
        "\n",
        "  with open(file_list[i],'r') as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "  clear_output(wait=False)\n",
        "  print(f\"{'Part1 - ' if 'Part1' in file_list[i] else 'Part2 - '}\", end='')\n",
        "  print(f\"{'NC - ' if 'NonClickbait' in file_list[i] else 'C - '}\", end='')\n",
        "  print(f\"{'Auto' if 'Auto' in file_list[i] else 'Direct'}\")\n",
        "\n",
        "  print(f\"입력 : {i}\")\n",
        "  print(f\"원문 제목:  {data.get('sourceDataInfo').get('newsTitle')}\")\n",
        "  print(f\"새로운 제목:  {data.get('labeledDataInfo').get('newTitle')}\")\n",
        "  print()\n",
        "\n",
        "  time.sleep(3.3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dA_uPxtj59N6"
      },
      "source": [
        "Training, Validation 폴더의 데이터는 다름,\n",
        "1:1로 나누어져있음"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kcWABtPB6CCh"
      },
      "outputs": [],
      "source": [
        "a = '/content/146.낚시성 기사 탐지 데이터/01.데이터/Training/02.라벨링데이터'\n",
        "b = '/content/146.낚시성 기사 탐지 데이터/01.데이터/Validation/02.라벨링데이터'\n",
        "sum=0\n",
        "for root, dirs, files in os.walk(a):\n",
        "  sum += len(files)\n",
        "\n",
        "print(\"a: \", sum)\n",
        "print()\n",
        "\n",
        "for root, dirs, files in os.walk(b):\n",
        "  sum+=len(files)\n",
        "print(\"b: \",sum)\n",
        "\n",
        "print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "leU0GHUm58k9"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bGoD4Unu4-a3"
      },
      "source": [
        "### **3. 데이터 준비**\n",
        "**낚시성 기사 분류 모델** 학습에 쓰일 데이터 준비 - Clickbait Nonclickbait 분류"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ReJP_ZUadBI"
      },
      "source": [
        "정제데이터를 저장할 폴더 다 만들기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y5-g7X5byFey",
        "outputId": "50602ad4-1552-48a4-90bd-30343c5a5ed5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/root 폴더를 만들었습니다.\n",
            "/content/root/training 폴더를 만들었습니다.\n",
            "/content/root/test 폴더를 만들었습니다.\n",
            "/content/root/training/nc 폴더를 만들었습니다.\n",
            "/content/root/training/c 폴더를 만들었습니다.\n",
            "/content/root/test/nc 폴더를 만들었습니다.\n",
            "/content/root/test/c 폴더를 만들었습니다.\n"
          ]
        }
      ],
      "source": [
        "#root - training&test - nc&c 폴더 다 만들기\n",
        "root_path = '/content/root'\n",
        "training_path = os.path.join(root_path,'training')\n",
        "test_path = os.path.join(root_path,'test')\n",
        "\n",
        "training_nc_path = os.path.join(training_path,'nc')\n",
        "training_c_path = os.path.join(training_path,'c')\n",
        "test_nc_path = os.path.join(test_path,'nc')\n",
        "test_c_path = os.path.join(test_path,'c')\n",
        "\n",
        "all_paths = [root_path, training_path, test_path, training_nc_path, training_c_path, test_nc_path, test_c_path]\n",
        "for path in all_paths:\n",
        "  if not os.path.exists(path):\n",
        "    os.makedirs(path)\n",
        "    print(f\"{path} 폴더를 만들었습니다.\")\n",
        "  else:\n",
        "    print(f\"{path} 폴더는 이미 존재합니다.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "keT7_85rUT2S"
      },
      "source": [
        "txt 파일로 변환하기 전 Clickbait NonClickbait 별로 폴더 정리"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "NtyedEQYfNvm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a31a3835-d6c6-4844-9027-43e9de7acf3b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/146.낚시성 기사 탐지 데이터/01.데이터/Training/02.라벨링데이터\n",
            "/content/146.낚시성 기사 탐지 데이터/01.데이터/Training/02.라벨링데이터/TL_Part2_Clickbait_Direct_ET\n",
            "/content/146.낚시성 기사 탐지 데이터/01.데이터/Training/02.라벨링데이터/TL_Part2_Clickbait_Auto_GB\n",
            "/content/146.낚시성 기사 탐지 데이터/01.데이터/Training/02.라벨링데이터/TL_Part1_Clickbait_Direct_EC\n",
            "/content/146.낚시성 기사 탐지 데이터/01.데이터/Training/02.라벨링데이터/TL_Part2_NonClickbait_Auto_IS\n",
            "/content/146.낚시성 기사 탐지 데이터/01.데이터/Training/02.라벨링데이터/TL_Part1_Clickbait_Direct_LC\n",
            "/content/146.낚시성 기사 탐지 데이터/01.데이터/Training/02.라벨링데이터/TL_Part1_Clickbait_Auto_ET\n",
            "/content/146.낚시성 기사 탐지 데이터/01.데이터/Training/02.라벨링데이터/TL_Part2_Clickbait_Direct_IS\n",
            "/content/146.낚시성 기사 탐지 데이터/01.데이터/Training/02.라벨링데이터/TL_Part1_NonClickbait_Auto_GB\n",
            "/content/146.낚시성 기사 탐지 데이터/01.데이터/Training/02.라벨링데이터/TL_Part2_Clickbait_Auto_LC\n",
            "/content/146.낚시성 기사 탐지 데이터/01.데이터/Training/02.라벨링데이터/TL_Part2_Clickbait_Auto_SO\n",
            "/content/146.낚시성 기사 탐지 데이터/01.데이터/Training/02.라벨링데이터/TL_Part1_Clickbait_Direct_SO\n",
            "/content/146.낚시성 기사 탐지 데이터/01.데이터/Training/02.라벨링데이터/TL_Part2_NonClickbait_Auto_SO\n",
            "/content/146.낚시성 기사 탐지 데이터/01.데이터/Training/02.라벨링데이터/TL_Part2_Clickbait_Auto_ET\n",
            "/content/146.낚시성 기사 탐지 데이터/01.데이터/Training/02.라벨링데이터/TL_Part1_Clickbait_Direct_GB\n",
            "/content/146.낚시성 기사 탐지 데이터/01.데이터/Training/02.라벨링데이터/TL_Part2_Clickbait_Auto_EC\n",
            "/content/146.낚시성 기사 탐지 데이터/01.데이터/Training/02.라벨링데이터/TL_Part2_Clickbait_Direct_GB\n",
            "/content/146.낚시성 기사 탐지 데이터/01.데이터/Training/02.라벨링데이터/TL_Part1_Clickbait_Auto_IS\n",
            "/content/146.낚시성 기사 탐지 데이터/01.데이터/Training/02.라벨링데이터/TL_Part2_Clickbait_Direct_EC\n",
            "/content/146.낚시성 기사 탐지 데이터/01.데이터/Training/02.라벨링데이터/TL_Part2_NonClickbait_Auto_EC\n",
            "/content/146.낚시성 기사 탐지 데이터/01.데이터/Training/02.라벨링데이터/TL_Part2_Clickbait_Direct_LC\n",
            "/content/146.낚시성 기사 탐지 데이터/01.데이터/Training/02.라벨링데이터/TL_Part1_Clickbait_Auto_PO\n",
            "/content/146.낚시성 기사 탐지 데이터/01.데이터/Training/02.라벨링데이터/TL_Part1_Clickbait_Direct_ET\n",
            "/content/146.낚시성 기사 탐지 데이터/01.데이터/Training/02.라벨링데이터/TL_Part1_NonClickbait_Auto_ET\n",
            "/content/146.낚시성 기사 탐지 데이터/01.데이터/Training/02.라벨링데이터/TL_Part1_Clickbait_Auto_GB\n",
            "/content/146.낚시성 기사 탐지 데이터/01.데이터/Training/02.라벨링데이터/TL_Part1_NonClickbait_Auto_EC\n",
            "/content/146.낚시성 기사 탐지 데이터/01.데이터/Training/02.라벨링데이터/TL_Part1_NonClickbait_Auto_IS\n",
            "/content/146.낚시성 기사 탐지 데이터/01.데이터/Training/02.라벨링데이터/TL_Part1_NonClickbait_Auto_SO\n",
            "/content/146.낚시성 기사 탐지 데이터/01.데이터/Training/02.라벨링데이터/TL_Part2_NonClickbait_Auto_GB\n",
            "/content/146.낚시성 기사 탐지 데이터/01.데이터/Training/02.라벨링데이터/TL_Part1_Clickbait_Direct_IS\n",
            "/content/146.낚시성 기사 탐지 데이터/01.데이터/Training/02.라벨링데이터/TL_Part2_NonClickbait_Auto_LC\n",
            "/content/146.낚시성 기사 탐지 데이터/01.데이터/Training/02.라벨링데이터/TL_Part1_NonClickbait_Auto_PO\n",
            "/content/146.낚시성 기사 탐지 데이터/01.데이터/Training/02.라벨링데이터/TL_Part2_Clickbait_Direct_PO\n",
            "/content/146.낚시성 기사 탐지 데이터/01.데이터/Training/02.라벨링데이터/TL_Part1_Clickbait_Auto_LC\n",
            "/content/146.낚시성 기사 탐지 데이터/01.데이터/Training/02.라벨링데이터/TL_Part1_NonClickbait_Auto_LC\n",
            "/content/146.낚시성 기사 탐지 데이터/01.데이터/Training/02.라벨링데이터/TL_Part2_Clickbait_Auto_IS\n",
            "/content/146.낚시성 기사 탐지 데이터/01.데이터/Training/02.라벨링데이터/TL_Part2_Clickbait_Direct_SO\n",
            "/content/146.낚시성 기사 탐지 데이터/01.데이터/Training/02.라벨링데이터/TL_Part1_Clickbait_Auto_SO\n",
            "/content/146.낚시성 기사 탐지 데이터/01.데이터/Training/02.라벨링데이터/TL_Part1_Clickbait_Direct_PO\n",
            "/content/146.낚시성 기사 탐지 데이터/01.데이터/Training/02.라벨링데이터/TL_Part2_NonClickbait_Auto_PO\n",
            "/content/146.낚시성 기사 탐지 데이터/01.데이터/Training/02.라벨링데이터/TL_Part2_NonClickbait_Auto_ET\n",
            "/content/146.낚시성 기사 탐지 데이터/01.데이터/Training/02.라벨링데이터/TL_Part2_Clickbait_Auto_PO\n",
            "/content/146.낚시성 기사 탐지 데이터/01.데이터/Training/02.라벨링데이터/TL_Part1_Clickbait_Auto_EC\n",
            "/content/146.낚시성 기사 탐지 데이터/01.데이터/Validation/02.라벨링데이터\n",
            "/content/146.낚시성 기사 탐지 데이터/01.데이터/Validation/02.라벨링데이터/VL_Part2_NonClickbait_Auto_LC\n",
            "/content/146.낚시성 기사 탐지 데이터/01.데이터/Validation/02.라벨링데이터/VL_Part2_Clickbait_Auto_ET\n",
            "/content/146.낚시성 기사 탐지 데이터/01.데이터/Validation/02.라벨링데이터/VL_Part2_NonClickbait_Auto_GB\n",
            "/content/146.낚시성 기사 탐지 데이터/01.데이터/Validation/02.라벨링데이터/VL_Part1_Clickbait_Direct_IS\n",
            "/content/146.낚시성 기사 탐지 데이터/01.데이터/Validation/02.라벨링데이터/VL_Part1_Clickbait_Auto_GB\n",
            "/content/146.낚시성 기사 탐지 데이터/01.데이터/Validation/02.라벨링데이터/VL_Part1_NonClickbait_Auto_ET\n",
            "/content/146.낚시성 기사 탐지 데이터/01.데이터/Validation/02.라벨링데이터/VL_Part2_Clickbait_Auto_EC\n",
            "/content/146.낚시성 기사 탐지 데이터/01.데이터/Validation/02.라벨링데이터/VL_Part2_Clickbait_Auto_PO\n",
            "/content/146.낚시성 기사 탐지 데이터/01.데이터/Validation/02.라벨링데이터/VL_Part1_NonClickbait_Auto_IS\n",
            "/content/146.낚시성 기사 탐지 데이터/01.데이터/Validation/02.라벨링데이터/VL_Part2_Clickbait_Direct_GB\n",
            "/content/146.낚시성 기사 탐지 데이터/01.데이터/Validation/02.라벨링데이터/VL_Part2_Clickbait_Direct_ET\n",
            "/content/146.낚시성 기사 탐지 데이터/01.데이터/Validation/02.라벨링데이터/VL_Part2_NonClickbait_Auto_IS\n",
            "/content/146.낚시성 기사 탐지 데이터/01.데이터/Validation/02.라벨링데이터/VL_Part1_Clickbait_Auto_EC\n",
            "/content/146.낚시성 기사 탐지 데이터/01.데이터/Validation/02.라벨링데이터/VL_Part1_NonClickbait_Auto_EC\n",
            "/content/146.낚시성 기사 탐지 데이터/01.데이터/Validation/02.라벨링데이터/VL_Part1_Clickbait_Auto_SO\n",
            "/content/146.낚시성 기사 탐지 데이터/01.데이터/Validation/02.라벨링데이터/VL_Part1_Clickbait_Auto_LC\n",
            "/content/146.낚시성 기사 탐지 데이터/01.데이터/Validation/02.라벨링데이터/VL_Part1_NonClickbait_Auto_PO\n",
            "/content/146.낚시성 기사 탐지 데이터/01.데이터/Validation/02.라벨링데이터/VL_Part2_Clickbait_Direct_SO\n",
            "/content/146.낚시성 기사 탐지 데이터/01.데이터/Validation/02.라벨링데이터/VL_Part2_Clickbait_Auto_LC\n",
            "/content/146.낚시성 기사 탐지 데이터/01.데이터/Validation/02.라벨링데이터/VL_Part1_NonClickbait_Auto_SO\n",
            "/content/146.낚시성 기사 탐지 데이터/01.데이터/Validation/02.라벨링데이터/VL_Part1_Clickbait_Auto_PO\n",
            "/content/146.낚시성 기사 탐지 데이터/01.데이터/Validation/02.라벨링데이터/VL_Part2_NonClickbait_Auto_PO\n",
            "/content/146.낚시성 기사 탐지 데이터/01.데이터/Validation/02.라벨링데이터/VL_Part1_Clickbait_Direct_LC\n",
            "/content/146.낚시성 기사 탐지 데이터/01.데이터/Validation/02.라벨링데이터/VL_Part2_Clickbait_Direct_EC\n",
            "/content/146.낚시성 기사 탐지 데이터/01.데이터/Validation/02.라벨링데이터/VL_Part1_Clickbait_Direct_PO\n",
            "/content/146.낚시성 기사 탐지 데이터/01.데이터/Validation/02.라벨링데이터/VL_Part1_Clickbait_Direct_ET\n",
            "/content/146.낚시성 기사 탐지 데이터/01.데이터/Validation/02.라벨링데이터/VL_Part1_Clickbait_Auto_IS\n",
            "/content/146.낚시성 기사 탐지 데이터/01.데이터/Validation/02.라벨링데이터/VL_Part2_Clickbait_Direct_PO\n",
            "/content/146.낚시성 기사 탐지 데이터/01.데이터/Validation/02.라벨링데이터/VL_Part2_Clickbait_Auto_IS\n",
            "/content/146.낚시성 기사 탐지 데이터/01.데이터/Validation/02.라벨링데이터/VL_Part1_Clickbait_Direct_GB\n",
            "/content/146.낚시성 기사 탐지 데이터/01.데이터/Validation/02.라벨링데이터/VL_Part2_NonClickbait_Auto_EC\n",
            "/content/146.낚시성 기사 탐지 데이터/01.데이터/Validation/02.라벨링데이터/VL_Part2_NonClickbait_Auto_ET\n",
            "/content/146.낚시성 기사 탐지 데이터/01.데이터/Validation/02.라벨링데이터/VL_Part1_Clickbait_Direct_SO\n",
            "/content/146.낚시성 기사 탐지 데이터/01.데이터/Validation/02.라벨링데이터/VL_Part2_Clickbait_Auto_SO\n",
            "/content/146.낚시성 기사 탐지 데이터/01.데이터/Validation/02.라벨링데이터/VL_Part1_Clickbait_Auto_ET\n",
            "/content/146.낚시성 기사 탐지 데이터/01.데이터/Validation/02.라벨링데이터/VL_Part1_NonClickbait_Auto_GB\n",
            "/content/146.낚시성 기사 탐지 데이터/01.데이터/Validation/02.라벨링데이터/VL_Part2_Clickbait_Direct_IS\n",
            "/content/146.낚시성 기사 탐지 데이터/01.데이터/Validation/02.라벨링데이터/VL_Part2_NonClickbait_Auto_SO\n",
            "/content/146.낚시성 기사 탐지 데이터/01.데이터/Validation/02.라벨링데이터/VL_Part1_Clickbait_Direct_EC\n",
            "/content/146.낚시성 기사 탐지 데이터/01.데이터/Validation/02.라벨링데이터/VL_Part1_NonClickbait_Auto_LC\n",
            "/content/146.낚시성 기사 탐지 데이터/01.데이터/Validation/02.라벨링데이터/VL_Part2_Clickbait_Direct_LC\n",
            "/content/146.낚시성 기사 탐지 데이터/01.데이터/Validation/02.라벨링데이터/VL_Part2_Clickbait_Auto_GB\n"
          ]
        }
      ],
      "source": [
        "orig_training_nc_path = []\n",
        "orig_training_c_path = []\n",
        "orig_test_nc_path = []\n",
        "orig_test_c_path = []\n",
        "\n",
        "for root,dirs,files in os.walk('/content/146.낚시성 기사 탐지 데이터/01.데이터/Training/02.라벨링데이터'):\n",
        "  print(root)\n",
        "  if 'NonClickbait' in root:\n",
        "    orig_training_nc_path.append(root)\n",
        "  else:\n",
        "    orig_training_c_path.append(root)\n",
        "\n",
        "for root,dirs,files in os.walk('/content/146.낚시성 기사 탐지 데이터/01.데이터/Validation/02.라벨링데이터'):\n",
        "  print(root)\n",
        "  if 'NonClickbait' in root:\n",
        "    orig_test_nc_path.append(root)\n",
        "  else:\n",
        "    orig_test_c_path.append(root)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UHG5YC-XUXqH"
      },
      "source": [
        "본문과 제목만 뺀 txt 파일로 넣습니다"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 145
        },
        "id": "XHaEAlWg_MFs",
        "outputId": "391d8101-dd1d-473c-e801-00ca61295058"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"json_to_txt('/content/146.낚시성 기사 탐지 데이터/01.데이터/Training/02.라벨링데이터/TL_Part1_NonClickbait_Auto_GB',\\n            training_nc_path )#t-nc\\njson_to_txt('/content/146.낚시성 기사 탐지 데이터/01.데이터/Training/02.라벨링데이터/TL_Part1_Clickbait_Auto_PO',\\n            training_c_path) #t_c\\njson_to_txt('/content/146.낚시성 기사 탐지 데이터/01.데이터/Validation/02.라벨링데이터/VL_Part1_NonClickbait_Auto_IS',\\n            test_nc_path) #test-nc\\njson_to_txt('/content/146.낚시성 기사 탐지 데이터/01.데이터/Validation/02.라벨링데이터/VL_Part1_Clickbait_Auto_IS',\\n            test_c_path) #test-c\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "import re\n",
        "#start_path 에 있는거 destination_path에 [숫자.txt]파일로 긁음\n",
        "def json_to_txt(start_path, destination_path,max):\n",
        "  file_list = []\n",
        "\n",
        "  org_training = start_path\n",
        "  for root,dirs,files in os.walk(org_training):\n",
        "    for file_name in files:\n",
        "      dir = os.path.join(root,file_name)\n",
        "      file_list.append(dir)\n",
        "\n",
        "  #각 파일의 필요한 것만 빼서 정제데이터에 저장합니다\n",
        "  length = str(len(file_list))\n",
        "\n",
        "  for file in file_list[:max]:\n",
        "\n",
        "    #print(str(file_list.index(file))+'/'+length)#진행상황\n",
        "\n",
        "    #경로에서 불러온 파일이 json파일이면 제목, 본문을 가져와 정제데이터에 저장\n",
        "    if file.endswith('.json'):\n",
        "      with open(file,'r') as f:\n",
        "        data = json.load(f)\n",
        "      title = data.get('sourceDataInfo').get('newsTitle')\n",
        "      content = data.get('sourceDataInfo').get('newsContent')\n",
        "      txt_title = re.sub('/','',title)\n",
        "      txt_path = os.path.join(destination_path, txt_title[:15]+'.txt')\n",
        "\n",
        "      #print(\"path : \" + txt_path)\n",
        "      with open(txt_path,'w',encoding='utf-8') as file:\n",
        "        file.write(title)\n",
        "        file.write(content)\n",
        "        file.close()\n",
        "\n",
        "#1000개씩만해도 폴더 28개에 28000개\n",
        "\n",
        "#'/content/146.낚시성 기사 탐지 데이터/01.데이터/Training/02.라벨링데이터' -> orig_t_nc & c 나누기\n",
        "#'/content/146.낚시성 기사 탐지 데이터/01.데이터/Validation/02.라벨링데이터' -> orig_ nc & c 나누기\n",
        "#orig_training_nc_path = []\n",
        "#orig_training_c_path = []\n",
        "#orig_test_nc_path = []\n",
        "#orig_test_c_path = []\n",
        "for i in orig_training_nc_path:\n",
        "  json_to_txt(i,training_nc_path,7500)\n",
        "for i in orig_training_c_path:\n",
        "  json_to_txt(i,training_c_path,7500)\n",
        "for i in orig_test_nc_path:\n",
        "  json_to_txt(i,test_nc_path,7500)\n",
        "for i in orig_test_c_path:\n",
        "  json_to_txt(i,test_c_path,7500)\n",
        "\n",
        "\n",
        "'''json_to_txt('/content/146.낚시성 기사 탐지 데이터/01.데이터/Training/02.라벨링데이터/TL_Part1_NonClickbait_Auto_GB',\n",
        "            training_nc_path )#t-nc\n",
        "json_to_txt('/content/146.낚시성 기사 탐지 데이터/01.데이터/Training/02.라벨링데이터/TL_Part1_Clickbait_Auto_PO',\n",
        "            training_c_path) #t_c\n",
        "json_to_txt('/content/146.낚시성 기사 탐지 데이터/01.데이터/Validation/02.라벨링데이터/VL_Part1_NonClickbait_Auto_IS',\n",
        "            test_nc_path) #test-nc\n",
        "json_to_txt('/content/146.낚시성 기사 탐지 데이터/01.데이터/Validation/02.라벨링데이터/VL_Part1_Clickbait_Auto_IS',\n",
        "            test_c_path) #test-c\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AnuZHTuthAkk"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "## 여기부턴 Single Classification Bert practice1 복사"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3BkNzSb7fk2i"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "import tensorflow_text as text\n",
        "from official.nlp import optimization #tf models official 에 있는 optimization 모델\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "tf.get_logger().setLevel('ERROR')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8A9bmROjR6E2",
        "outputId": "b7d2d061-41d7-4241-a2a2-10841b2a725b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 272657 files belonging to 2 classes.\n",
            "Using 218126 files for training.\n",
            "Found 272657 files belonging to 2 classes.\n",
            "Using 54531 files for validation.\n",
            "Found 61794 files belonging to 2 classes.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "['c', 'nc']"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "batch_size = 200 #학습시간 줄이려고 잠깐 크게함\n",
        "seed = 42\n",
        "\n",
        "#train dataset\n",
        "###text_dataset_from_directory 를 사용해서 aclImdb/train의 데이터를 로드함. 20%는 validation, 80%는 training 에 쓰고 한 번에 얼마나 학습할 것인지 결정하는 batch size는 32로, 일관된 작업을 위해서 seed를 설정해줌.\n",
        "raw_train_ds = tf.keras.utils.text_dataset_from_directory(\n",
        "    training_path,\n",
        "    batch_size=batch_size,\n",
        "    validation_split=0.2,\n",
        "    subset='training',\n",
        "    seed=seed)\n",
        "\n",
        "#validation dataset\n",
        "#? validation 데이터셋에 왜 validaton split을 또 해주는지 : 위에서 train dataset을 만들고 남은 20%를 사용하겠다는 의미이다. subset의 값은 training, validation 두 개로 정해져있고 validation으로 지정할 경우 이렇게 쓰는것이다.\n",
        "val_ds = tf.keras.utils.text_dataset_from_directory(\n",
        "    training_path,\n",
        "    batch_size=batch_size,\n",
        "    validation_split=0.2,\n",
        "    subset='validation',\n",
        "    seed=seed)\n",
        "\n",
        "#test dataset\n",
        "#? seed랑 subset 설정 왜 안함? : subset을 안해준 이유는 test데이터는 test데이터이기 때문에, seed는 테스트 데이터에서는 일관되게 처리하는게 딱히 중요하지 않기 때문\n",
        "test_ds = tf.keras.utils.text_dataset_from_directory(\n",
        "    test_path,\n",
        "    batch_size=batch_size)\n",
        "\n",
        "#데이터셋을 메모리에 캐시함.(그냥 로딩 효율 높히기)\n",
        "#buffer_size AUTOTUNE : Tensorflow의 기능. 모델이 학습하는 동안 다음 배치를 미리 준비함.\n",
        "train_ds = raw_train_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
        "val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
        "test_ds = test_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
        "\n",
        "#raw_train_ds 데이터셋에서 클래스 이름 추출하기 (폴더 이름을 기반으로 함)\n",
        "#이 경우엔 neg, pos\n",
        "class_names = raw_train_ds.class_names\n",
        "class_names"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "fE-eFmDWR6CZ"
      },
      "outputs": [],
      "source": [
        "#for text_batch, label_batch in train_ds.take(1):\n",
        "  #for i in range(3):\n",
        "    #print(f'Review: {text_batch.numpy()[i]}')\n",
        "    #label = label_batch.numpy()[i]\n",
        "    #print(f'Label : {label} ({class_names[label]})')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rW0_ScTdha03"
      },
      "outputs": [],
      "source": [
        "tfhub_handle_preprocess = 'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3'\n",
        "tfhub_handle_encoder = 'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-512_A-8/1'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6NFVoPfghMfJ"
      },
      "outputs": [],
      "source": [
        "#Enrie Model - pretrained & BERT 에 dropout과 classifier을 더해 전체 모델 선언\n",
        "def build_classifier_model():\n",
        "  preprocessing_layer = hub.KerasLayer(tfhub_handle_preprocess, name='preprocessing')\n",
        "  encoder = hub.KerasLayer(tfhub_handle_encoder, trainable=True, name='BERT_encoder') #trainable - 학습 중에 모델 조정 가능\n",
        "  #과적합 방지 위한 dropout Layer\n",
        "  #? 인수는 무엇을 위한 값인가? : dropout rate 지정\n",
        "  dropout = tf.keras.layers.Dropout(0.1)\n",
        "  #최종 출력을 위한 분류기 Layer\n",
        "  classifier = tf.keras.layers.Dense(1, activation=None, name='classifier')\n",
        "\n",
        "  #text 입력해 preprocessing, encoding 거침 -> 나중에 모델을 불러오기 했을 때 text 를 prediction() 메소드에 입력해주어야 함. 형식 : text = ['문자열']\n",
        "  text_input = tf.keras.layers.Input(shape=(), dtype=tf.string, name='text')\n",
        "  encoder_inputs = preprocessing_layer(text_input)\n",
        "  outputs = encoder(encoder_inputs)\n",
        "\n",
        "  #encoding 결과물의 pooled_output을 dropout, classifier 적용\n",
        "  net = outputs['pooled_output']\n",
        "  net = dropout(net)\n",
        "  net = classifier(net)\n",
        "\n",
        "  #입력값, 결과값 keras Model로 반환\n",
        "  return tf.keras.Model(text_input, net)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D6ishZ-jhbon",
        "outputId": "02afddd3-5e61-4315-908b-612d6ec8a021"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tf.Tensor([[0.7183222]], shape=(1, 1), dtype=float32)\n"
          ]
        }
      ],
      "source": [
        "#text를 tf.constant로 tensor 형태로 변환한 후 전체 모델을 통해 통과시킨 결과를 sigmoid 함수로 확률값으로 나타낸다\n",
        "classifier_model = build_classifier_model()\n",
        "bert_raw_result = classifier_model(tf.constant(['아주 좋아요 이거']))\n",
        "print(tf.sigmoid(bert_raw_result))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aPfR8JRghisJ"
      },
      "source": [
        "Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zfoZB7PwhiQb"
      },
      "outputs": [],
      "source": [
        "#이진분류 classification의 loss와 accuracy 저장하는 tf 객체 생성, .numpy()시 각 수치 리턴\n",
        "loss = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
        "metrics = tf.metrics.BinaryAccuracy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aDvILTzWhmpk"
      },
      "outputs": [],
      "source": [
        "epochs = 8\n",
        "#train dataset's cardinality .numpy()를 통해 값 return 받음 : 하나하나 다 하겠다는 의미\n",
        "steps_per_epoch = tf.data.experimental.cardinality(train_ds).numpy()\n",
        "num_train_steps = steps_per_epoch * epochs\n",
        "#0.1만큼 warmup을 가지며 optimizer가 데이터에 적응할 수 있도록 해줌\n",
        "num_warmup_steps = int(0.1*num_train_steps)\n",
        "#warmup끝나고 목표로 하는 학습 비율\n",
        "init_lr = 3e-5\n",
        "\n",
        "optimizer = optimization.create_optimizer(init_lr=init_lr,\n",
        "                                          num_train_steps=num_train_steps,\n",
        "                                          num_warmup_steps=num_warmup_steps,\n",
        "                                          optimizer_type='adamw') #optimizer adamw\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HCi03-G0VCac"
      },
      "source": [
        "코랩 런타임 다운 안되게\n",
        "function ClickConnect(){\n",
        "console.log(\"Working\");\n",
        "document.querySelector(\"colab-toolbar-button#connect\").click()\n",
        "}setInterval(ClickConnect, 1800000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TwISIdKvhpdk",
        "outputId": "fa165f50-2cf1-4a99-ee43-1934f0dfe3b8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training model with https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-512_A-8/1\n",
            "Epoch 1/8\n",
            "1091/1091 [==============================] - 5173s 5s/step - loss: 0.6354 - binary_accuracy: 0.6780 - val_loss: 0.6127 - val_binary_accuracy: 0.6801\n",
            "Epoch 2/8\n",
            "1091/1091 [==============================] - 5145s 5s/step - loss: 0.6132 - binary_accuracy: 0.6823 - val_loss: 0.6027 - val_binary_accuracy: 0.6841\n",
            "Epoch 3/8\n",
            "1091/1091 [==============================] - 5077s 5s/step - loss: 0.5997 - binary_accuracy: 0.6871 - val_loss: 0.5984 - val_binary_accuracy: 0.6896\n",
            "Epoch 4/8\n",
            " 259/1091 [======>.......................] - ETA: 53:50 - loss: 0.5923 - binary_accuracy: 0.6911"
          ]
        }
      ],
      "source": [
        "#위에서 만든 loss, accuracy, optimizer 사용해 전체 모델 compile\n",
        "classifier_model.compile(optimizer = optimizer,\n",
        "                         loss=loss,\n",
        "                         metrics=metrics)\n",
        "\n",
        "print(f'Training model with {tfhub_handle_encoder}')\n",
        "history = classifier_model.fit(x=train_ds,\n",
        "                               validation_data = val_ds,\n",
        "                               epochs = epochs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g9fLVz8Nhx8r",
        "outputId": "883d6cd8-80ec-4f5a-94fb-e8b6a878a8be"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "183/183 [==============================] - 599s 3s/step - loss: 0.6280 - binary_accuracy: 0.6747\n",
            "Loss : 0.6280061602592468\n",
            "Accuracy: 0.6746793389320374\n"
          ]
        }
      ],
      "source": [
        "#지표 테스트\n",
        "loss, accuracy = classifier_model.evaluate(test_ds)\n",
        "\n",
        "print(f'Loss : {loss}')\n",
        "print(f'Accuracy: {accuracy}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UKYFP7aMjBKD"
      },
      "outputs": [],
      "source": [
        "#모델 저장하기\n",
        "folder_path = '/content/my_model'\n",
        "if not os.path.exists(folder_path):\n",
        "  os.mkdir(folder_path)\n",
        "else:\n",
        "  print(\"folder exists\")\n",
        "\n",
        "model_path = os.path.join(folder_path,'first_model')\n",
        "classifier_model.save(model_path) #모델 + weight 통째로 저장\n",
        "\n",
        "#모델 폴더 압축하기(PC 저장용)\n",
        "import zipfile\n",
        "\n",
        "folder_path = '/content/my_model'\n",
        "zip_path = '/content/drive/MyDrive/my_model.zip'\n",
        "\n",
        "#zip으로 저\n",
        "with zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
        "        for root, dirs, files in os.walk(folder_path):\n",
        "            for file in files:\n",
        "                file_path = os.path.join(root, file)\n",
        "                arcname = os.path.relpath(file_path, folder_path)\n",
        "                zipf.write(file_path, arcname=arcname)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g9ZT2_BJeYyG"
      },
      "source": [
        "모델 저장하고 테스트"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9qhNmeNhaIoj"
      },
      "outputs": [],
      "source": [
        "zip_path = '/content/drive/MyDrive/ML_Data/my_model.zip'\n",
        "\n",
        "unzip(zip_path,'/content')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fYH__xjdeYOA"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import load_model\n",
        "model_path = '/content/first_model'\n",
        "\n",
        "#모델 불러오기 - AdamW는 checkpoint를 지원하지 않아 모델을 load하고 다시 컴파일을 할 수 없으니 오류가 난다. compile=False를 지정해주어서 나는 그걸 안할것이라고 명시해주어야 한다.\n",
        "model = load_model(model_path,compile=False)\n",
        "\n",
        "#model = Model()\n",
        "#model.load_weight(weight_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 169
        },
        "id": "sxrN4EWteEgD",
        "outputId": "049c30d5-d829-440e-a204-12c20d650525"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-75dc3a12-a280-4a15-b52d-e1b3a6fa917b\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-75dc3a12-a280-4a15-b52d-e1b3a6fa917b\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving 3.txt to 3.txt\n",
            "이 게임 술게임하면서 봤습니다. 자면서 봤습니다. 아무것도 하지 않으면서도 봤습니다. 그런데도 내 머릿속에 꼴아박히는 줄거리와 그 이미지는 지울수가 없었습니다. 스피커와 모니터를 뚫고 나올 것 같은 화면 연출.\n",
            "1/1 [==============================] - 1s 652ms/step\n",
            "[-0.195556] c\n",
            "신뢰도45.12662124633789%로, 낚시성 기사입니다\n"
          ]
        }
      ],
      "source": [
        "from google.colab import files\n",
        "import io\n",
        "\n",
        "# 파일 업로드\n",
        "uploaded = files.upload()\n",
        "\n",
        "# 업로드된 파일 중 txt 파일을 선택하여 내용을 읽음\n",
        "for fn in uploaded.keys():\n",
        "  if fn.endswith('.txt'):\n",
        "    # 파일 열기\n",
        "    with io.open(fn, 'r', encoding='utf-8') as file:\n",
        "      text = file.read()\n",
        "      break\n",
        "\n",
        "# 변수 'text'에 저장된 내용 확인\n",
        "print(text)\n",
        "text = [text]\n",
        "\n",
        "#model을 만드는 함수를 지정할 때 입력할 데이터의 이름을 'text'라고 내가 선언해주었다. 형식 : text = ['문자열']\n",
        "prediction = model.predict(text)\n",
        "\n",
        "res = 'nc' if prediction[0] >=0.5 else 'c'\n",
        "percentage = tf.sigmoid(prediction[0][0])\n",
        "\n",
        "print(prediction[0],res)\n",
        "if res=='nc':\n",
        "  print(f'신뢰도 {percentage*100}%로, 낚시성 기사가 아닙니다')\n",
        "else:\n",
        "  print(f'신뢰도{percentage*100}%로, 낚시성 기사입니다')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EoZryNRieGyN",
        "outputId": "27e8b269-60e5-453b-968a-cedef0e1a879"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<keras.src.engine.input_layer.InputLayer object at 0x7edcfead7c70>\n",
            "text\n",
            "\n",
            "<keras.src.saving.legacy.saved_model.load.KerasLayer object at 0x7edcfead73a0>\n",
            "preprocessing\n",
            "\n",
            "<keras.src.saving.legacy.saved_model.load.KerasLayer object at 0x7edcfead4700>\n",
            "BERT_encoder\n",
            "\n",
            "<keras.src.layers.regularization.dropout.Dropout object at 0x7edcfead6020>\n",
            "dropout\n",
            "\n",
            "<keras.src.layers.core.dense.Dense object at 0x7edcfead7460>\n",
            "classifier\n",
            "\n"
          ]
        }
      ],
      "source": [
        "for layers in model.layers:\n",
        "  print(layers)\n",
        "  print(layers.name)\n",
        "  print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hkBpIO2zhJ6L"
      },
      "source": [
        "CPU로 할 때\n",
        "1000개씩 하면 5시간 걸림 300분\n",
        "200개씩 하면 1시간 걸림 X 10epoch\n",
        "\n",
        "->30배 속도\n",
        "\n",
        "GPU로하니\n",
        "1000개씩 할 때 10분\n",
        "7500개씩 할 때 75분 X 8 epoch\n",
        "10000개씩 할 때 100분 X 5 epoch\n",
        "20000개씩 할 때 (다) 200분"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "TQWjl9MIs7PO"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}