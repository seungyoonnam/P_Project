{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## 시작\n",
        "패키지 install, 함수 정의"
      ],
      "metadata": {
        "id": "OJPSTDzyzIhe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://wikidocs.net/192931\n"
      ],
      "metadata": {
        "id": "VkU_XpyovwZ2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install jpype1"
      ],
      "metadata": {
        "id": "b-JVCazvsFYe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2065b1f2-7dcb-4f0c-f171-cffd413f4056"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: jpype1 in /usr/local/lib/python3.10/dist-packages (1.4.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from jpype1) (23.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install konlpy"
      ],
      "metadata": {
        "id": "OgiAX981sG0Y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3971b8e1-0a00-4780-d3cb-241117125060"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: konlpy in /usr/local/lib/python3.10/dist-packages (0.6.0)\n",
            "Requirement already satisfied: JPype1>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from konlpy) (1.4.1)\n",
            "Requirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.10/dist-packages (from konlpy) (4.9.3)\n",
            "Requirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.10/dist-packages (from konlpy) (1.23.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from JPype1>=0.7.0->konlpy) (23.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install sentencepiece"
      ],
      "metadata": {
        "id": "EL4wlS_mtOrL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "db4e7bdf-2758-4d47-96d3-46d8bb1719b9"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (0.1.99)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#한글파일 unzip시 깨짐 encoding decoding으로 방지하는 함수, num개만 압축해제\n",
        "def unzip(source_path, dest_path):\n",
        "  with zipfile.ZipFile(source_path, 'r') as zf:\n",
        "    zipInfo = zf.infolist()\n",
        "    for member in zipInfo:\n",
        "      try:\n",
        "        #print(member.filename.encode('cp437').decode('euc-kr','ignore'))\n",
        "        member.filename = member.filename.encode('cp437').decode('euc-kr','ignore')\n",
        "        zf.extract(member,dest_path)\n",
        "      except:\n",
        "        print(source_path)\n",
        "        raise Exception('??')"
      ],
      "metadata": {
        "id": "XbnSxMtmE93w"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 데이터 준비\n",
        "데이터 다운로드"
      ],
      "metadata": {
        "id": "yRCeHJvpAavu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "OZeSzFer91zV"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "import shutil\n",
        "import zipfile\n",
        "import os\n",
        "\n",
        "import json"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/drive')\n",
        "\n",
        "source_path = '/content/drive/MyDrive/ML_Data/146.낚시성 기사 탐지 데이터.zip'\n",
        "destination_path = '/content/146.낚시성 기사 탐지 데이터.zip'\n",
        "shutil.copyfile(source_path,destination_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "MCgltYRr94D1",
        "outputId": "dd147c70-bfc9-4c0c-f62b-8bd8e8631f15"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/146.낚시성 기사 탐지 데이터.zip'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data 다운로드/압축해제\n",
        "원문 Clickbait Direct 만 뽑아 새로운 데이터셋 생성 - 본문/제목 다중입력"
      ],
      "metadata": {
        "id": "SbbbErUHyu2r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#데이터 저장을 위한 경로 만들기\n",
        "root_path = '/content/root'\n",
        "training_data_path = os.path.join(root_path,'Training')\n",
        "test_data_path = os.path.join(root_path,'Test')\n",
        "\n",
        "p = [root_path,training_data_path,test_data_path]\n",
        "for i in p:\n",
        "  if not os.path.exists(i):\n",
        "    os.mkdir(i)"
      ],
      "metadata": {
        "id": "W4Q4xqlgFYag"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#전체 중 Clickbait_Direct 아닌거 삭제 - Clickbait_Direct 만 processPattern 가짐\n",
        "unzip('/content/146.낚시성 기사 탐지 데이터.zip','/content')\n",
        "raw_data_path = '/content/146.낚시성 기사 탐지 데이터'\n",
        "\n",
        "for root,dirs,files in os.walk(raw_data_path):\n",
        "  for fn in files:\n",
        "    if 'Clickbait_Direct' not in fn:\n",
        "      os.remove(root+'/'+fn)\n",
        "\n"
      ],
      "metadata": {
        "id": "uguErh06BvaU"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#(임시) zip 하나만 풀어서 dataset 만들기\n",
        "a='/content/146.낚시성 기사 탐지 데이터/01.데이터/Training/02.라벨링데이터/TL_Part1_Clickbait_Direct_EC.zip'\n",
        "b='/content/146.낚시성 기사 탐지 데이터/01.데이터/Training/02.라벨링데이터/TL_Part1_Clickbait_Direct_IS.zip'\n",
        "unzip(a,training_data_path)\n",
        "unzip(b,test_data_path)"
      ],
      "metadata": {
        "id": "G8lCSqPEin6o"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 데이터 전처리"
      ],
      "metadata": {
        "id": "aG8_NSJry4LF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "본문->토큰화->벡터화(원-핫 인코딩)\n",
        "\n",
        "1.   train_data\n",
        "[본문.........],[본문..........]....\n",
        "2.   token_train_data\n",
        "[시퀀스시퀀스시퀀스],[시퀀스시퀀스시퀀스],[시퀀스시퀀스시퀀스]....\n",
        "3. x_train\n",
        "[0,0,0,0,1,0,0,,,,],[0,1,0,0,0,0,,,,,,],,,,,\n",
        "\n"
      ],
      "metadata": {
        "id": "OG5sgl_7jcPJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "데이터준비"
      ],
      "metadata": {
        "id": "TV3i624EAIHO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import numpy as np\n",
        "\n",
        "#training_data 경로의 모든 파일 읽어오기 (json)\n",
        "training_files = []\n",
        "for root, dirs, files in os.walk(training_data_path):\n",
        "  for fn in files:\n",
        "    training_files.append(root+'/'+fn)\n",
        "\n",
        "#각 json 파일의 본문을 리스트에 추가\n",
        "train_data = np.array([])\n",
        "train_labels = []\n",
        "\n",
        "for i in training_files[:10]:\n",
        "  with open(i,'r',encoding='utf-8') as file:\n",
        "    data = json.load(file)\n",
        "\n",
        "    newsContent = data.get('sourceDataInfo').get('newsContent')\n",
        "    #newsContent = re.sub(r'\\n',' ', newsContent) # 엔터키를 없애버리면 sentencepiece 패키지가 멍청해서 인식을 못함\n",
        "    newsContent = re.sub('[()]',' ',newsContent)\n",
        "    newsContent = re.sub(r'\\\\',' ',newsContent)\n",
        "    train_data = np.append(newsContent,train_data)\n",
        "\n",
        "    train_labels.append(int(data.get('sourceDataInfo').get('processPattern')))\n",
        "\n",
        "#-------------------------------------------------------------------------\n",
        "#위에랑 똑같은거 test에도\n",
        "test_files = []\n",
        "for root,dirs,files in os.walk(test_data_path):\n",
        "  for fn in files:\n",
        "    test_files.append(root+'/'+fn)\n",
        "\n",
        "test_data = np.array([])\n",
        "test_labels = []\n",
        "\n",
        "for i in test_files[:10]:\n",
        "  with open(i,'r',encoding = 'utf-8') as file:\n",
        "    data = json.load(file)\n",
        "\n",
        "    newsContent = data.get('sourceDataInfo').get('newsContent')\n",
        "    newsContent = re.sub('[()]',' ',newsContent)\n",
        "    newsContent = re.sub(r'\\\\',' ',newsContent)\n",
        "    test_data = np.append(newsContent,test_data)\n",
        "\n",
        "    test_labels.append(int(data.get('sourceDataInfo').get('processPattern')))"
      ],
      "metadata": {
        "id": "vHDKQUMtkcZ6"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "본문 - 토큰화"
      ],
      "metadata": {
        "id": "G62H-jtO-E3X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#한국어 토큰화 위한 과정\n",
        "#https://velog.io/@lighthouse97/%ED%85%8D%EC%8A%A4%ED%8A%B8-%EC%A0%84%EC%B2%98%EB%A6%AC-%ED%86%A0%ED%81%B0%ED%99%94\n",
        "\n",
        "#토큰화 - 1 형태소 분석\n",
        "from konlpy.tag import Okt\n",
        "\n",
        "okt = Okt()\n",
        "text = '아이고 배고파 ㅠㅠ 이럴수가. 컵라면을 사먹어요'\n",
        "\n",
        "morphs = okt.morphs(text)\n",
        "ps = okt.pos(text)\n",
        "noun = okt.nouns(text)\n",
        "print(f\"형태소 분석 : {morphs}\")\n",
        "print(f'품사 태깅 : {ps}')\n",
        "print(f'명사 추출 : {noun}')"
      ],
      "metadata": {
        "id": "c7e1Hb5rr2kb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#토큰화 - 2 Subword Tokenizer\n",
        "#BPE 알고리즘을 구현한 구글의 Sentencepiece를 사용해 내부 단어 분리 진행\n",
        "\n",
        "#이름.model 이 sentencepiece에서 사용되는 단어집합\n",
        "#이름.vocal 이 단어 집합을 텍스트 파일로 저장한것\n",
        "#모든 텍스트파일을 하나의 txt에 쳐넣고 학습해 model을 만들고 각각의 train_data 안에 있는 데이터에 sp.encode를 해주면 된다.\n",
        "\n",
        "import sentencepiece as spm\n",
        "#모든 텍스트 하나의 txt에 쳐넣어 sentencepiece 학습시키기\n",
        "with open('/content/test1.txt','w',encoding='utf-8') as f:\n",
        "  for i in range(0,len(train_data)):\n",
        "    f.write(train_data[i])\n",
        "  for i in range(0,len(test_data)):\n",
        "    f.write(test_data[i])\n",
        "\n",
        "spm.SentencePieceTrainer.Train('--input=test1.txt --model_prefix=mine --vocab_size=5000 --model_type=bpe --max_sentence_length=9999')\n",
        "\n",
        "#Sentence piece 객체 생성\n",
        "sp = spm.SentencePieceProcessor()\n",
        "\n"
      ],
      "metadata": {
        "id": "d7RG29bQn0IN"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#모델 불러와 인코딩, 디코딩 시험\n",
        "sp.load('mine.model')\n",
        "print(sp.encode(train_data[0], out_type=int))\n",
        "print(sp.DecodeIds([3864, 2255, 4515, 392, 556, 1254, 2030, 1084, 1415, 4282, 1912, 2764, 1925, 392, 556, 916, 3247, 2780, 100, 1237, 4282, 2901, 4088, 51, 2394, 3512, 4349, 4284, 288, 43, 189, 4328, 1077, 3482, 392, 1983, 1255]))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6j5loIVTt9CA",
        "outputId": "aa1bcb3e-615b-4b66-dfb1-68907c45c533"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[3864, 2255, 4515, 392, 556, 1254, 2030, 1084, 1415, 4282, 1912, 2764, 1925, 392, 556, 916, 3247, 2780, 100, 1237, 4282, 2901, 4088, 51, 2394, 3512, 4349, 4284, 288, 43, 189, 4328, 1077, 3482, 392, 1983, 1255, 1198, 98, 1742, 2019, 1177, 4003, 2417, 100, 4279, 4091, 4282, 1541, 451, 1192, 392, 556, 916, 1255, 514, 2045, 259, 615, 3694, 1983, 1198, 818, 451, 2019, 1177, 3604, 3855, 4282, 3695, 556, 292, 430, 1901, 433, 1208, 4282, 1192, 1912, 1913, 2894, 4287, 1594, 2776, 278, 392, 556, 1254, 1176, 2100, 1177, 934, 471, 1742, 3583, 3379, 1081, 1356, 1901, 1198, 3268, 100, 810, 157, 4282, 2989, 290, 4341, 2045, 3291, 954, 4302, 916, 4257, 100, 2177, 612, 4282, 846, 759, 2312, 1422, 290, 346, 2496, 1201, 451, 916, 2603, 2099, 803, 656, 11, 421, 1569, 3290, 2862, 3591, 1255, 433, 1391, 1208, 4282, 44, 2822, 2850, 151, 114, 3718, 2186, 1756, 1130, 108, 556, 1254, 3258, 4295, 2030, 3951, 22, 1530, 4472, 71, 114, 4280, 515, 3686, 4036, 3588, 1966, 1707, 4472, 4292, 150, 4282]\n",
            "한동안 치솟던 아파트 매매가와 전세가가 진정세로 돌아섰다. 지난주 대전과 충남의 아파트 매매가와 전세가는 안정을 되찾은 것으로 나타났다. 부동산정보업체 '부동산써브'에 따르면 지난 4일 기준 전국의 아파트 매매가는 전주대비\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#train_data, test_data 전부 Tokenize\n",
        "\n",
        "token_train_data = []\n",
        "for i in range(0,len(train_data)):\n",
        "  token_train_data.append(sp.encode(train_data[i], out_type = int))\n",
        "token_test_data = []\n",
        "for i in range(0,len(test_data))  :\n",
        "  token_test_data.append(sp.encode(test_data[i],out_type=int))"
      ],
      "metadata": {
        "id": "I6rnpfXCwtqN"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "본문 - 벡터변환 (원-핫 인코딩)"
      ],
      "metadata": {
        "id": "1s1tjGR0wWgk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#데이터 벡터변환 (원-핫 인코딩 방식 벡터변환으로 각 기사에 어떤 단어가 사용되었는지에 대한 정보를 담고 있다.)\n",
        "import numpy as np\n",
        "\n",
        "#행 : 각 기사들, 열 : 단어 시퀀스, 값 : 기사의 해당 단어 사용 여부\n",
        "def vectorize_squences(sequences, dimension=5000):\n",
        "  results = np.zeros((len(sequences),dimension))\n",
        "  print(results.shape)\n",
        "\n",
        "  #i번째 원소의 기사 sequence를 np.zero 각각의 줄에 그냥 다 넣기\n",
        "  for i, sequence in enumerate(sequences):\n",
        "    results[i,sequence] = 1\n",
        "  return results\n",
        "\n",
        "x_train = vectorize_squences(token_train_data)\n",
        "x_test = vectorize_squences(token_test_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I9qHHVuWwRzc",
        "outputId": "362c0402-3cc5-4181-f264-fb0e7679c101"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(10, 5000)\n",
            "(10, 5000)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "라벨->벡터변환(원 핫 인코딩)\n",
        "\n",
        "\n",
        "1.   train_labels\n",
        "[3,4,3,6,1,2,6,3,7,3....]\n",
        "2.   one_hot_train_labels\n",
        "[0,0,0,0,1,0,0,0],[0,0,0,0,0,0,0,1],[1,0,0,0,0,0,0,0]\n"
      ],
      "metadata": {
        "id": "ngGMmXlt-L5Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "라벨 값\n",
        "*   0-5은 processPattern 11-16\n",
        "*   6-9은 processPattern 21-24\n"
      ],
      "metadata": {
        "id": "_FTyeuK5CqDW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#라벨 원-핫 인코딩 벡터변환\n",
        "def to_one_hot(labels, dimension=10):\n",
        "  results = np.zeros((len(labels),dimension))\n",
        "  for i, label in enumerate(labels):\n",
        "    if label <= 16:\n",
        "      results[i,label-11] = 1\n",
        "    elif label >=21:\n",
        "      results[i,label-15] = 1\n",
        "    else:\n",
        "      pass\n",
        "  return results\n",
        "\n",
        "one_hot_train_labels = to_one_hot(train_labels)\n",
        "one_hot_test_labels = to_one_hot(test_labels)"
      ],
      "metadata": {
        "id": "oYLqcSnF_Tcj"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(one_hot_train_labels)\n",
        "print(one_hot_test_labels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j1na8biRDXgz",
        "outputId": "5367c8e1-babd-47b2-8d9d-3c2b742bdb9e"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
            " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
            "[[1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
            " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.모델 만들기\n",
        "\n",
        "\n",
        "입력 Layer 2개- 본문/제목\n",
        "은닉 Layer\n",
        "출력 Layer 10개\n"
      ],
      "metadata": {
        "id": "GWurdSxeBstI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#시퀀스-벡터 한 데이터 1:9로 나누기\n",
        "x_val = x_train[:5]\n",
        "partial_x_train = x_train[5:]\n",
        "\n",
        "#label one_hot_encoding 한 데이터 1:9로 나누기\n",
        "y_val = one_hot_train_labels[:5]\n",
        "partial_y_train = one_hot_train_labels[5:]"
      ],
      "metadata": {
        "id": "KJcXbb-I__3k"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. 모델 구성\n",
        "출력 클래스 개수 10개"
      ],
      "metadata": {
        "id": "M6G1C7L2CdCq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras import models\n",
        "from keras import layers\n",
        "\n",
        "#Sequential 모델 정의\n",
        "model = models.Sequential()\n",
        "#입력층 - 64개의 뉴런, relu 활성화 함수, 10000개의 입력데이터\n",
        "model.add(layers.Dense(64, activation='relu',input_shape=(5000,)))\n",
        "#은닉층\n",
        "model.add(layers.Dense(64, activation='relu'))\n",
        "#출력층\n",
        "model.add(layers.Dense(10, activation='softmax'))\n",
        "\n",
        "\n",
        "model.compile(optimizer='rmsprop',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "bEe3_4M8Cfdh"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#keras Sequential model에 나눈 데이터 중 9쪽 데이터를 넣어 train하고 validation으로는 1쪽을 넣음\n",
        "history = model.fit(partial_x_train,\n",
        "                    partial_y_train,\n",
        "                    epochs=20,\n",
        "                    batch_size=1,\n",
        "                    validation_data=(x_val,y_val))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rN9_1l51Dncp",
        "outputId": "56e02f9b-6700-424c-8411-748ed071ebd5"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "5/5 [==============================] - 1s 62ms/step - loss: 2.1662 - accuracy: 0.2000 - val_loss: 1.9411 - val_accuracy: 0.2000\n",
            "Epoch 2/20\n",
            "5/5 [==============================] - 0s 14ms/step - loss: 1.0327 - accuracy: 1.0000 - val_loss: 1.6332 - val_accuracy: 0.6000\n",
            "Epoch 3/20\n",
            "5/5 [==============================] - 0s 13ms/step - loss: 0.4098 - accuracy: 1.0000 - val_loss: 1.5068 - val_accuracy: 0.6000\n",
            "Epoch 4/20\n",
            "5/5 [==============================] - 0s 16ms/step - loss: 0.1739 - accuracy: 1.0000 - val_loss: 1.3999 - val_accuracy: 0.6000\n",
            "Epoch 5/20\n",
            "5/5 [==============================] - 0s 14ms/step - loss: 0.0900 - accuracy: 1.0000 - val_loss: 1.3589 - val_accuracy: 0.6000\n",
            "Epoch 6/20\n",
            "5/5 [==============================] - 0s 16ms/step - loss: 0.0514 - accuracy: 1.0000 - val_loss: 1.3129 - val_accuracy: 0.8000\n",
            "Epoch 7/20\n",
            "5/5 [==============================] - 0s 19ms/step - loss: 0.0322 - accuracy: 1.0000 - val_loss: 1.2950 - val_accuracy: 0.8000\n",
            "Epoch 8/20\n",
            "5/5 [==============================] - 0s 21ms/step - loss: 0.0211 - accuracy: 1.0000 - val_loss: 1.2708 - val_accuracy: 0.8000\n",
            "Epoch 9/20\n",
            "5/5 [==============================] - 0s 18ms/step - loss: 0.0142 - accuracy: 1.0000 - val_loss: 1.2516 - val_accuracy: 0.8000\n",
            "Epoch 10/20\n",
            "5/5 [==============================] - 0s 19ms/step - loss: 0.0098 - accuracy: 1.0000 - val_loss: 1.2308 - val_accuracy: 0.8000\n",
            "Epoch 11/20\n",
            "5/5 [==============================] - 0s 13ms/step - loss: 0.0070 - accuracy: 1.0000 - val_loss: 1.2218 - val_accuracy: 0.8000\n",
            "Epoch 12/20\n",
            "5/5 [==============================] - 0s 15ms/step - loss: 0.0051 - accuracy: 1.0000 - val_loss: 1.2138 - val_accuracy: 0.8000\n",
            "Epoch 13/20\n",
            "5/5 [==============================] - 0s 15ms/step - loss: 0.0036 - accuracy: 1.0000 - val_loss: 1.2106 - val_accuracy: 0.8000\n",
            "Epoch 14/20\n",
            "5/5 [==============================] - 0s 16ms/step - loss: 0.0026 - accuracy: 1.0000 - val_loss: 1.2025 - val_accuracy: 0.8000\n",
            "Epoch 15/20\n",
            "5/5 [==============================] - 0s 13ms/step - loss: 0.0019 - accuracy: 1.0000 - val_loss: 1.1942 - val_accuracy: 0.8000\n",
            "Epoch 16/20\n",
            "5/5 [==============================] - 0s 17ms/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 1.1897 - val_accuracy: 0.8000\n",
            "Epoch 17/20\n",
            "5/5 [==============================] - 0s 17ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 1.1815 - val_accuracy: 0.8000\n",
            "Epoch 18/20\n",
            "5/5 [==============================] - 0s 13ms/step - loss: 8.0271e-04 - accuracy: 1.0000 - val_loss: 1.1785 - val_accuracy: 0.8000\n",
            "Epoch 19/20\n",
            "5/5 [==============================] - 0s 17ms/step - loss: 6.0578e-04 - accuracy: 1.0000 - val_loss: 1.1782 - val_accuracy: 0.8000\n",
            "Epoch 20/20\n",
            "5/5 [==============================] - 0s 16ms/step - loss: 4.5678e-04 - accuracy: 1.0000 - val_loss: 1.1770 - val_accuracy: 0.8000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pred = model.predict(x_test)\n",
        "np.argmax(pred[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A8-8v9VLDxjQ",
        "outputId": "1f1259a8-f92a-4a5c-a7a0-b93ab25e7eca"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 22ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "새로운 데이터로 테스트해보기\n",
        "본문->토큰화->벡터화->모델 분석 ->argmax 함수"
      ],
      "metadata": {
        "id": "y1O50FtdF-uE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = '''강도형 해양수산부 장관 후보자가 후쿠시마 오염수 방류와 관련해 수산물 안전을 확보하는데 최선을 다하겠다는 뜻을 밝혔다.\n",
        "\n",
        "7일 강도형 후보자는 지명 이후 처음 서울 마포구에 마련된 인사 청문 준비 사무실로 출근했다.\n",
        "\n",
        "강 후보자는 수산물 안전 우려를 묻는 취재진의 질문에 \"국민에게 위해가 되는 일이 없게 하겠다\"며 \"국내외의 검사 지점을 활용해 방사능 검사를 계속하겠다\"고 말했다.\n",
        "\n",
        "또 \"항만 분야는 배후 부지를 활용해 경제적 가치를 가질 수 있도록 하고 그린쉽 친환경선박을 잘 챙길 것\"이라고 했다.\n",
        "\n",
        "강 후보자는 \"해양수산 관련 산업구조를 첨단화해 국제사회 선도 국가로 갈 수 있는 전략을 짜야 한다면서 \"소통하면서 타 부처와 협업할 것\"이라고 말했다.\n",
        "\n",
        "정부의 R&D 예산 삭감에 대해서는 \"이번 연구개발 개편은 비효율적인 것을 걷어내는 작업이었다고 생각한다\"며 \"부족한 것이 있다면 2025년, 2026년 예산에 넣어 보강하도록 노력하겠다\"고 덧붙였다.\n",
        "\n",
        "강 후보자는 해양과학 전문 연구원 출신으로 지난 2월부터 한국해양과학기술원(KIOST) 원장을 맡아왔다.'''\n",
        "\n",
        "token_text = sp.encode(text,out_type=int)\n",
        "vec_text = vectorize_squences(token_text)\n",
        "pred1 = model.predict(vec_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6bvo6NiLE-gu",
        "outputId": "f86425b8-b8cc-4450-d5a6-075b028337cd"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[3864, 2255, 4515, 392, 556, 1254, 2030, 1084, 1415, 4282, 1912, 2764, 1925, 392, 556, 916, 3247, 2780, 100, 1237, 4282, 2901, 4088, 51, 2394, 3512, 4349, 4284, 288, 43, 189, 4328, 1077, 3482, 392, 1983, 1255, 1198, 98, 1742, 2019, 1177, 4003, 2417, 100, 4279, 4091, 4282, 1541, 451, 1192, 392, 556, 916, 1255, 514, 2045, 259, 615, 3694, 1983, 1198, 818, 451, 2019, 1177, 3604, 3855, 4282, 3695, 556, 292, 430, 1901, 433, 1208, 4282, 1192, 1912, 1913, 2894, 4287, 1594, 2776, 278, 392, 556, 1254, 1176, 2100, 1177, 934, 471, 1742, 3583, 3379, 1081, 1356, 1901, 1198, 3268, 100, 810, 157, 4282, 2989, 290, 4341, 2045, 3291, 954, 4302, 916, 4257, 100, 2177, 612, 4282, 846, 759, 2312, 1422, 290, 346, 2496, 1201, 451, 916, 2603, 2099, 803, 656, 11, 421, 1569, 3290, 2862, 3591, 1255, 433, 1391, 1208, 4282, 44, 2822, 2850, 151, 114, 3718, 2186, 1756, 1130, 108, 556, 1254, 3258, 4295, 2030, 3951, 22, 1530, 4472, 71, 114, 4280, 515, 3686, 4036, 3588, 1966, 1707, 4472, 4292, 150, 4282]\n",
            "(316, 5000)\n",
            "10/10 [==============================] - 0s 4ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.argmax(pred1[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H-etMSWOF1OR",
        "outputId": "52a48723-bed1-4e92-9e39-c4c754349520"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 83
        }
      ]
    }
  ]
}