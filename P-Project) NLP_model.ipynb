{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "386b424b",
   "metadata": {},
   "source": [
    "원천 데이터와 라벨링 데이터가 있는데\n",
    "\n",
    "Part1은 제목과 본문이 불일치 기사이며 Part2는 본문의 도메인 일관성 부족 기사이다.\n",
    "\n",
    "CA = 낚시성_자동생성, CD = 낚시성_직접생성, NA = 비낚시성_자동생성이다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ff1f316b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7e548f75",
   "metadata": {},
   "outputs": [],
   "source": [
    "Training_raw_path = 'C:/Users/user/Desktop/과제/data/146.낚시성 기사 탐지 데이터/01.데이터/Training/01.원천데이터/'\n",
    "Training_labeling_path = 'C:/Users/user/Desktop/과제/data/146.낚시성 기사 탐지 데이터/01.데이터/Training/02.라벨링데이터/'\n",
    "Validation_raw_path = 'C:/Users/user/Desktop/과제/data/146.낚시성 기사 탐지 데이터/01.데이터/Validation/01.원천데이터/'\n",
    "Validation_labeling_path = 'C:/Users/user/Desktop/과제/data/146.낚시성 기사 탐지 데이터/01.데이터/Validation/02.라벨링데이터/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ce594bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "Training_raw_list = []\n",
    "Training_labeling_list = []\n",
    "Validation_raw_list = []\n",
    "Validation_labeling_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5a388551",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 데이터 다 읽어오는 함수\n",
    "# 근데 AI 허브 홈페이지에서랑 내가 읽는 파일의 갯수랑 다르다...\n",
    "def process_read_file(directory_path, data_list):\n",
    "    for file_name in os.listdir(directory_path):\n",
    "        if file_name.endswith('.zip'):\n",
    "            zip_file_path = os.path.join(directory_path, file_name)\n",
    "\n",
    "            with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
    "                # ZIP 파일 내의 모든 파일 목록\n",
    "                file_list = zip_ref.namelist()\n",
    "\n",
    "                # 각 JSON 파일에 접근\n",
    "                for file_name in file_list:\n",
    "                    # JSON 파일인 경우에만 처리\n",
    "                    if file_name.endswith('.json'):\n",
    "                        with zip_ref.open(file_name) as json_file:\n",
    "                            # JSON 파일 읽기\n",
    "                            json_data = json.load(json_file)\n",
    "\n",
    "                            # 필요한 정보에 접근\n",
    "                            news_id = json_data['sourceDataInfo']['newsID']\n",
    "                            news_type = json_data['sourceDataInfo'][\"useType\"] # 1 = 진실,  2 = 거짓\n",
    "                            news_title = json_data['sourceDataInfo']['newsTitle']\n",
    "                            news_content = json_data['sourceDataInfo']['newsContent']\n",
    "                            news_category = json_data['sourceDataInfo']['newsCategory']\n",
    "                            news_sentence_count = json_data['sourceDataInfo']['sentenceCount']\n",
    "                            \n",
    "                            #라벨링 데이터는 'clickbaitClass 추가'\n",
    "                            if 'labeledDataInfo' in json_data and 'clickbaitClass' in json_data['labeledDataInfo']:\n",
    "                                clickbaitClass = json_data['labeledDataInfo']['clickbaitClass']\n",
    "                                \n",
    "                                data_list.append({\n",
    "                                    'FileName': news_id,\n",
    "                                    'NewsType' : news_type,\n",
    "                                    'NewsTitle': news_title,\n",
    "                                    'NewsCategory' : news_category,\n",
    "                                    'NewsContent': news_content,\n",
    "                                    'SentenceCount' : news_sentence_count,\n",
    "                                    'clickbaitClass' : clickbaitClass,\n",
    "                                })\n",
    "                            #raw 데이터\n",
    "                            else:\n",
    "                                data_list.append({\n",
    "                                    'FileName': news_id,\n",
    "                                    'NewsType' : news_type,\n",
    "                                    'NewsTitle': news_title,\n",
    "                                    'NewsCategory' : news_category,\n",
    "                                    'NewsContent': news_content,\n",
    "                                    'SentenceCount' : news_sentence_count,\n",
    "                                })\n",
    "                            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a80b0d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "process_read_file(Training_raw_path, Training_raw_list)\n",
    "process_read_file(Training_labeling_path, Training_labeling_list)\n",
    "process_read_file(Validation_raw_path, Validation_raw_list)\n",
    "process_read_file(Validation_labeling_path, Validation_labeling_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "36d6d1f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 각 데이터를 df로 만들어 주기\n",
    "Training_raw_df = pd.DataFrame(Training_raw_list)\n",
    "Training_labeling_df = pd.DataFrame(Training_labeling_list)\n",
    "Validation_raw_df = pd.DataFrame(Validation_raw_list)\n",
    "Validation_labeling_df = pd.DataFrame(Validation_labeling_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e7abcddd",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "429122 586724 53627 73321\n"
     ]
    }
   ],
   "source": [
    "#   AI 허브에서의 데이터 갯수 raw part1 = 364,333, raw part2 = 369,094, validate part1 = 364,333, validate part1 = 369,094\n",
    "# 완전 다르쥬..\n",
    "print(len(Training_raw_df), len(Training_labeling_df), len(Validation_raw_df), len(Validation_labeling_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a9e7254d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#FileName 기준으로 병합\n",
    "merged_training  = pd.merge(Training_raw_df, Training_labeling_df, on='FileName', how='inner')\n",
    "merged_validation = pd.merge(Validation_raw_df, Validation_labeling_df, on='FileName', how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "51f5aef6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "426020 52004\n"
     ]
    }
   ],
   "source": [
    "print(len(merged_training), len(merged_validation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "71632625",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 시간이 오래 걸려서 샘플링\n",
    "sampled_training_data = merged_training.sample(frac=0.01, random_state=42)\n",
    "sampled_validation_data = merged_validation.sample(frac=0.01, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "47a3745b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4260 520\n"
     ]
    }
   ],
   "source": [
    "print(len(sampled_training_data), len(sampled_validation_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a535660a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['FileName', 'NewsType_x', 'NewsTitle_x', 'NewsCategory_x',\n",
       "       'NewsContent_x', 'SentenceCount_x', 'NewsType_y', 'NewsTitle_y',\n",
       "       'NewsCategory_y', 'NewsContent_y', 'SentenceCount_y', 'clickbaitClass'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 칼럼을 정리해야하는데 일단 넘김\n",
    "merged_training.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d2a55091",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:408: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['가서', '같은', '것과', '결과에', '결론을', '관계가', '관련이', '그런', '그럼에도', '그렇게', '그에', '그치지', '김에', '까닭에', '낫다', '년도', '논하지', '누가', '다시', '달려', '대로', '대해', '되는', '되다', '되어', '들면', '들자면', '듯하다', '따르는', '따름이다', '따지지', '때가', '만은', '만이', '많은', '말하면', '말할것도', '몰라도', '몰랏다', '못하다', '미치다', '바꾸어서', '바꿔', '밖에', '방면으로', '보면', '보아', '부류의', '비길수', '비추어', '뿐만', '사람들', '상대적으로', '생각이다', '서술한바와같이', '쓰여', '아니다', '아니라', '안다', '안된다', '않고', '않기', '않는다면', '않다', '않다면', '않도록', '않으면', '알겠는가', '어쩔수', '없고', '없다', '예를', '외에', '요만한', '우에', '위에서', '이렇게', '이로', '이르다', '이와', '이유는', '인하여', '임에', '점에서', '정도에', '정도의', '종합한것과같이', '주저하지', '줄은', '지경이다', '틀림없다', '편이', '하고', '하기', '하기만', '하는', '하는것만', '하는것이', '하다', '하면', '하지', '한하다', '할수록', '함으로써', '해도', '해서는', '형식으로', '힘이'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# 형태소 분별\n",
    "from konlpy.tag import Okt\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# 사용자 정의 불용어 목록을 파일에서 읽어와 리스트로 저장\n",
    "with open('C:/Users/user/Desktop/과제/data/stop_words.txt', 'r', encoding='utf-8') as file:\n",
    "    stop_words = [line.strip() for line in file]\n",
    "    \n",
    "# 형태소 분석을 위한 Konlpy의 Okt 객체 생성\n",
    "okt = Okt()\n",
    "\n",
    "# 형태소 분석 함수 정의\n",
    "def tokenizer(text):\n",
    "    return okt.morphs(text)\n",
    "\n",
    "# 전처리: 각 뉴스 기사의 내용을 형태소로 분할하고 불용어를 제거하여 문자열로 재구성\n",
    "sampled_training_data['NewsContent_x'] = sampled_training_data['NewsContent_x'].apply(lambda x: ' '.join(tokenizer(x)))\n",
    "sampled_validation_data['NewsContent_x'] = sampled_validation_data['NewsContent_x'].apply(lambda x: ' '.join(tokenizer(x)))\n",
    "\n",
    "# TF-IDF 벡터화: TfidfVectorizer를 사용하여 텍스트 데이터를 TF-IDF 행렬로 변환\n",
    "vectorizer = TfidfVectorizer(stop_words=stop_words, max_features=1000)\n",
    "X_train = vectorizer.fit_transform(sampled_training_data['NewsContent_x'])\n",
    "X_val = vectorizer.transform(sampled_validation_data['NewsContent_x'])\n",
    "\n",
    "y_train = sampled_training_data['clickbaitClass']\n",
    "y_val = sampled_validation_data['clickbaitClass']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "00b354e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "134/134 [==============================] - 9s 32ms/step - loss: 0.6512 - accuracy: 0.6491 - val_loss: 0.6306 - val_accuracy: 0.6846\n",
      "Epoch 2/5\n",
      "134/134 [==============================] - 3s 25ms/step - loss: 0.6392 - accuracy: 0.6502 - val_loss: 0.6367 - val_accuracy: 0.6865\n",
      "Epoch 3/5\n",
      "134/134 [==============================] - 4s 27ms/step - loss: 0.5991 - accuracy: 0.6836 - val_loss: 0.6808 - val_accuracy: 0.5654\n",
      "Epoch 4/5\n",
      "134/134 [==============================] - 3s 24ms/step - loss: 0.5311 - accuracy: 0.7415 - val_loss: 0.6950 - val_accuracy: 0.5885\n",
      "Epoch 5/5\n",
      "134/134 [==============================] - 3s 23ms/step - loss: 0.4816 - accuracy: 0.7655 - val_loss: 0.7292 - val_accuracy: 0.5885\n",
      "17/17 [==============================] - 1s 9ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.35      0.37      0.36       164\n",
      "           1       0.70      0.69      0.70       356\n",
      "\n",
      "    accuracy                           0.59       520\n",
      "   macro avg       0.53      0.53      0.53       520\n",
      "weighted avg       0.59      0.59      0.59       520\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Bidirectional\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# 토큰화를 위한 Tokenizer 생성\n",
    "tokenizer = Tokenizer(num_words=1000, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(sampled_training_data['NewsContent_x'])\n",
    "\n",
    "# 훈련 데이터와 검증 데이터의 텍스트를 시퀀스로 변환\n",
    "X_train_sequences = tokenizer.texts_to_sequences(sampled_training_data['NewsContent_x'])\n",
    "X_val_sequences = tokenizer.texts_to_sequences(sampled_validation_data['NewsContent_x'])\n",
    "\n",
    "# 시퀀스를 패딩하여 길이를 일정하게 맞춤\n",
    "X_train_padded = pad_sequences(X_train_sequences, maxlen=100, padding='post', truncating='post')\n",
    "X_val_padded = pad_sequences(X_val_sequences, maxlen=100, padding='post', truncating='post')\n",
    "\n",
    "# 모델 정의\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=1000, output_dim=16, input_length=100))\n",
    "model.add(Bidirectional(LSTM(64)))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# 모델 컴파일\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# 모델 훈련\n",
    "model.fit(X_train_padded, y_train, epochs=5, validation_data=(X_val_padded, y_val))\n",
    "\n",
    "# 훈련된 모델 평가\n",
    "y_pred_prob = model.predict(X_val_padded)\n",
    "y_pred = (y_pred_prob > 0.5).astype(int)  # 확률이 0.5보다 크면 1, 아니면 0으로 이진화\n",
    "\n",
    "print(classification_report(y_val, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e260382",
   "metadata": {},
   "source": [
    "샘플링, 최대 피쳐 개수, epoch랑 층이 많이 낮아서 accuracy가 많이 낮다.\n",
    "\n",
    "하이퍼 파라미터만 조정하면 좀 올라가겠는데 학습을 하루 종일 해야한다. \n",
    "\n",
    "여기서 라벨링 데이터의 진실, 거짓 뿐만 아니라 각 패턴 별로 원핫 인코딩 진행하고 모델 학습하면 개량 가능성도 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb3c3720",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
